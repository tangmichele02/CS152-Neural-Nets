# Project Proposal

## Project Name: 
Ethical Investigation: Reducing Inductive Risk in Neural Networks

## Project Description
In my Philosophy of Science class, we read a paper called "Inductive Risk and Values in Science" by Heather Douglas. Her argument was that because scientists take on inductive risk (which she says is the "risk of error"), they must consider non-epistemic values (which she says is "social, ethical, political" values) when examining various parts of the scientific process, including, problem selection and methodological choice (choosing a level of statistical significance and choosing "which kinds of errors one is willing to tolerate"). 
I want to examine how inductive risk and non-epistemic values play a role in the development of neural networks. Especially given the level of opacity of decision making with neural networks, developers must consider their methodological choices, what errors might arise, and how to tune different parameters to minimize inductive risk. For example, when creating a model to detect cancer, false positives might be preferred over false negatives, whereas when creating a model to classify credit applicants' risk level might have to consider different types of error (these examples come from Karaca's article, see references). 
I also want to examine how different ML models might carry different levels of inductive risk and how that risk can be addressed. Additionally, I might create a technical component where I choose a model and tune its parameters in different ways to determine what risk outcomes are produced. Finally, I want to research and come up with various recommendations for addressing risk mitigation. 

## Goals
	- learn about the philosophical model of inductive risk and how it might apply to neural networks 
	- examine different kinds of ML models (ex. binary classification, deep learning, etc.) and determining what kinds of inductive risk they carry
	- make recommendations for reducing inductive risk 
  - create a technical component where we analyze how tuning models in different ways changes risk outcomes 

## References 
J. B. Biddle, “On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in Machine Learning,” Canadian Journal of Philosophy, vol. 52, no. 3, pp. 321–341, 2022. https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A
H. Douglas, "Inductive risk and values in science. Philosophy of science", vol. 67, no. 4, pp. 559-579. 2000. https://www.jstor.org/stable/pdf/188707.pdf
K. Karaca, "Values and inductive risk in machine learning modelling: the case of binary classification models," Euro Jnl Phil Sci, vol. 11, no. 102, 2021. https://doi.org/10.1007/s13194-021-00405-1
A. Kasirzadeh, "Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence," 2021. arXiv preprint arXiv:2103.00752.
E. Sullivan, "Understanding from machine learning models," The British Journal for the Philosophy of Science, 2022. https://www.journals.uchicago.edu/doi/full/10.1093/bjps/axz035

